#!/usr/bin/env python3
"""
è‡ªåŠ¨éƒ¨ç½²è„šæœ¬ï¼šå°†ç¿»è¯‘åçš„æ–‡ç« éƒ¨ç½²åˆ°blogæˆ–guidesç›®å½•ï¼Œå¹¶æ›´æ–°ä¸»é¡µå’Œç´¢å¼•é¡µ

åŠŸèƒ½ï¼š
1. æ‰«ætranslated_articlesç›®å½•ä¸­çš„HTMLæ–‡ä»¶
2. æå–æ–‡ç« å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€æè¿°ã€æ—¥æœŸç­‰ï¼‰
3. æ ¹æ®é…ç½®æˆ–å†…å®¹åˆ¤æ–­æ˜¯blogè¿˜æ˜¯guides
4. å°†æ–‡ä»¶ç§»åŠ¨åˆ°å¯¹åº”ç›®å½•
5. æ›´æ–°index.htmlã€blog/index.htmlå’Œguides/index.htmlä¸­çš„æ–‡ç« åˆ—è¡¨
6. ä¿æŒæœ€æ–°çš„æ–‡ç« åœ¨ä¸»é¡µæ˜¾ç¤º
7. åˆ é™¤å·²éƒ¨ç½²çš„æ–‡ç« å¹¶ä»ç´¢å¼•é¡µä¸­ç§»é™¤

ç”¨æ³•ï¼š
    # æ–¹å¼1ï¼šè‡ªåŠ¨åˆ¤æ–­blogæˆ–guidesï¼ˆæ¨èï¼‰
    python3 mcp/deploy.py --auto

    # æ–¹å¼2ï¼šæŒ‡å®šéƒ¨ç½²åˆ°blogç›®å½•
    python3 mcp/deploy.py --target blog

    # æ–¹å¼3ï¼šæŒ‡å®šéƒ¨ç½²åˆ°guidesç›®å½•
    python3 mcp/deploy.py --target guides

    # æ–¹å¼4ï¼šéƒ¨ç½²å•ä¸ªæ–‡ä»¶
    python3 mcp/deploy.py --file translated_articles/article.html --target blog

    # æ–¹å¼5ï¼šæŒ‡å®šæºç›®å½•
    python3 mcp/deploy.py --source-dir mcp/translated_articles --auto

    # æ–¹å¼6ï¼šåˆ é™¤æ–‡ç« 
    python3 mcp/deploy.py --delete blog/article.html
    python3 mcp/deploy.py --delete guides/article.html
    python3 mcp/deploy.py --delete article.html --target blog
"""

import os
import re
import sys
import argparse
import shutil
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Tuple
import json

# é…ç½®
BLOG_DIR = Path(__file__).parent.parent / 'blog'
GUIDES_DIR = Path(__file__).parent.parent / 'guides'
INDEX_HTML = Path(__file__).parent.parent / 'index.html'
BLOG_INDEX = BLOG_DIR / 'index.html'
GUIDES_INDEX = GUIDES_DIR / 'index.html'
TRANSLATED_DIR = Path(__file__).parent / 'translated_articles'

# Blogå’ŒGuidesçš„å…³é”®è¯ï¼ˆç”¨äºè‡ªåŠ¨åˆ†ç±»ï¼‰
BLOG_KEYWORDS = ['story', 'experience', 'adventure', 'journey', 'trip', 'travel', 'personal', 
                 'narrative', 'tale', 'moment', 'encounter', 'memory', 'å›å¿†', 'æ•…äº‹', 'ç»å†']
GUIDES_KEYWORDS = ['guide', 'how to', 'tutorial', 'tips', 'advice', 'information', 'complete',
                   'essential', 'visa', 'transport', 'app', 'vpn', 'food', 'ordering',
                   'æ”»ç•¥', 'æŒ‡å—', 'å¦‚ä½•', 'æ•™ç¨‹', 'ä¿¡æ¯']

# ä¸»é¡µæ˜¾ç¤ºçš„æ–‡ç« æ•°é‡
MAX_HOMEPAGE_STORIES = 5
MAX_HOMEPAGE_GUIDES = 5


class ArticleMetadata:
    """æ–‡ç« å…ƒæ•°æ®"""
    def __init__(self, file_path: Path):
        self.file_path = file_path
        self.title = ""
        self.description = ""
        self.date = datetime.now().strftime('%B %d, %Y')
        self.read_time = "10 min read"
        self.location = ""
        self.category = "TRAVEL STORY"
        self.icon = "ğŸ“–"
        self.content_preview = ""
        self.source_url = ""
        
    def extract_from_html(self) -> bool:
        """ä»HTMLæ–‡ä»¶ä¸­æå–å…ƒæ•°æ®"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_tag = soup.find('title')
            if title_tag:
                self.title = title_tag.get_text().replace(' - Travel-China.Help', '').strip()
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                self.title = re.sub(r'\*\*', '', self.title)  # ç§»é™¤markdownæ ¼å¼
            
            # ä»article-titleç±»ä¸­æå–æ ‡é¢˜ï¼ˆæ›´å‡†ç¡®ï¼‰
            article_title = soup.find(class_='article-title')
            if article_title:
                self.title = article_title.get_text().strip()
            
            # æå–æè¿°
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                self.description = meta_desc.get('content').strip()
            
            # æå–æ—¥æœŸ
            meta_date = soup.find(class_='meta-item')
            if meta_date:
                date_text = meta_date.get_text()
                # å°è¯•æå–æ—¥æœŸ
                date_match = re.search(r'(\w+ \d{1,2}, \d{4})', date_text)
                if date_match:
                    self.date = date_match.group(1)
            
            # æå–å†…å®¹é¢„è§ˆï¼ˆå‰200ä¸ªå­—ç¬¦ï¼‰
            content_div = soup.find(class_='article-content')
            if content_div:
                text = content_div.get_text(strip=True)
                self.content_preview = text[:200] + '...' if len(text) > 200 else text
                # ä¼°ç®—é˜…è¯»æ—¶é—´ï¼ˆå‡è®¾æ¯åˆ†é’Ÿ200å­—ï¼‰
                word_count = len(text.split())
                self.read_time = f"{max(3, word_count // 200)} min read"
            
            # æå–æºURL
            source_info = soup.find(class_='source-info')
            if source_info:
                source_link = source_info.find('a')
                if source_link:
                    self.source_url = source_link.get('href', '')
            
            # å¦‚æœæ²¡æœ‰æè¿°ï¼Œä½¿ç”¨å†…å®¹é¢„è§ˆ
            if not self.description:
                self.description = self.content_preview[:150] + '...' if len(self.content_preview) > 150 else self.content_preview
            
            return True
        except Exception as e:
            print(f"âŒ æå–å…ƒæ•°æ®å¤±è´¥ {self.file_path}: {e}")
            return False
    
    def determine_category(self) -> str:
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹åˆ¤æ–­æ˜¯blogè¿˜æ˜¯guides"""
        title_lower = self.title.lower()
        desc_lower = self.description.lower()
        content_lower = self.content_preview.lower()
        
        # æ£€æŸ¥guideså…³é”®è¯
        for keyword in GUIDES_KEYWORDS:
            if keyword in title_lower or keyword in desc_lower or keyword in content_lower:
                return 'guides'
        
        # æ£€æŸ¥blogå…³é”®è¯
        for keyword in BLOG_KEYWORDS:
            if keyword in title_lower or keyword in desc_lower or keyword in content_lower:
                return 'blog'
        
        # é»˜è®¤ï¼šå¦‚æœåŒ…å«"guide"ã€"how"ã€"tutorial"ç­‰ï¼Œå½’ä¸ºguides
        if any(word in title_lower for word in ['guide', 'how to', 'tutorial', 'tips', 'visa', 'app']):
            return 'guides'
        
        # å¦åˆ™å½’ä¸ºblog
        return 'blog'
    
    def get_icon(self) -> str:
        """æ ¹æ®æ ‡é¢˜å†…å®¹é€‰æ‹©åˆé€‚çš„å›¾æ ‡"""
        title_lower = self.title.lower()
        
        icon_map = {
            'visa': 'ğŸ›‚', 'passport': 'ğŸ›‚',
            'train': 'ğŸš„', 'rail': 'ğŸš„', 'transport': 'ğŸš„',
            'app': 'ğŸ“±', 'wechat': 'ğŸ“±', 'alipay': 'ğŸ“±',
            'vpn': 'ğŸŒ', 'internet': 'ğŸŒ',
            'food': 'ğŸœ', 'restaurant': 'ğŸœ', 'dining': 'ğŸœ', 'hotpot': 'ğŸŒ¶ï¸',
            'mountain': 'â›°ï¸', 'hiking': 'ğŸ¥¾', 'climb': 'â›°ï¸',
            'city': 'ğŸ›ï¸', 'beijing': 'ğŸ›ï¸', 'shanghai': 'ğŸŒƒ', 'chengdu': 'ğŸ¼',
            'story': 'ğŸ“–', 'experience': 'â¤ï¸', 'adventure': 'ğŸ¥¾',
            'funny': 'ğŸ˜‚', 'humor': 'ğŸ˜‚',
            'culture': 'ğŸ­', 'cultural': 'ğŸ­',
        }
        
        for keyword, icon in icon_map.items():
            if keyword in title_lower:
                return icon
        
        return 'ğŸ“–'  # é»˜è®¤å›¾æ ‡


def safe_filename(title: str) -> str:
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '', title)
    filename = re.sub(r'\s+', '_', filename)
    filename = filename.strip('_')
    # é™åˆ¶é•¿åº¦
    if len(filename) > 100:
        filename = filename[:100]
    return filename or 'article'


def extract_article_section(html_content: str, section_class: str) -> List[str]:
    """ä»HTMLä¸­æå–æ–‡ç« åˆ—è¡¨éƒ¨åˆ†"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨å®¹å™¨
    article_list = soup.find(class_='article-list')
    if not article_list:
        return []
    
    # æå–æ‰€æœ‰æ–‡ç« é¡¹
    items = article_list.find_all(class_='article-list-item', recursive=False)
    return [str(item) for item in items]


def insert_article_to_list(html_content: str, article_html: str, max_items: int = None) -> str:
    """å°†æ–°æ–‡ç« æ’å…¥åˆ°æ–‡ç« åˆ—è¡¨çš„å¼€å¤´"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨å®¹å™¨
    article_list = soup.find(class_='article-list')
    if not article_list:
        return html_content
    
    # è§£ææ–°æ–‡ç« HTML
    new_article = BeautifulSoup(article_html, 'html.parser')
    new_item = new_article.find(class_='article-list-item')
    if not new_item:
        return html_content
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    
    # ç§»é™¤æ‰€æœ‰ç°æœ‰é¡¹
    for item in existing_items:
        item.decompose()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    article_list.insert(0, new_item)
    
    # é‡æ–°æ’å…¥ç°æœ‰æ–‡ç« ï¼ˆé™åˆ¶æ•°é‡ï¼‰
    if max_items:
        for item in existing_items[:max_items - 1]:
            article_list.append(item)
    else:
        for item in existing_items:
            article_list.append(item)
    
    return str(soup)


def generate_article_list_item(article: ArticleMetadata, target_dir: str, from_index: str = 'root') -> str:
    """ç”Ÿæˆæ–‡ç« åˆ—è¡¨é¡¹çš„HTML
    
    Args:
        article: æ–‡ç« å…ƒæ•°æ®
        target_dir: ç›®æ ‡ç›®å½• (blog/guides)
        from_index: ä»å“ªä¸ªç´¢å¼•é¡µè°ƒç”¨ ('root', 'blog', 'guides')
    """
    filename = safe_filename(article.title) + '.html'
    
    # æ ¹æ®è°ƒç”¨ä½ç½®ç¡®å®šç›¸å¯¹è·¯å¾„
    if from_index == 'root':
        # ä»index.htmlè°ƒç”¨ï¼Œéœ€è¦å®Œæ•´è·¯å¾„
        relative_path = f"{target_dir}/{filename}"
    else:
        # ä»blog/index.htmlæˆ–guides/index.htmlè°ƒç”¨ï¼Œåªéœ€è¦æ–‡ä»¶å
        relative_path = filename
    
    # æ ¹æ®ç±»åˆ«é€‰æ‹©æ ‡ç­¾æ ·å¼
    tag_class = "tag-story"
    if 'guide' in article.title.lower() or 'visa' in article.title.lower():
        tag_class = "tag-guide"
    elif 'food' in article.title.lower() or 'hotpot' in article.title.lower():
        tag_class = "tag-food"
    elif 'adventure' in article.title.lower() or 'hiking' in article.title.lower():
        tag_class = "tag-adventure"
    
    return f'''            <div class="article-list-item" onclick="window.location.href='{relative_path}'">
                <div class="article-icon">{article.get_icon()}</div>
                <div class="article-info">
                    <span class="article-tag {tag_class}">{article.category}</span>
                    <h3>{article.title}</h3>
                    <p>{article.description}</p>
                    <div class="article-meta-compact">
                        <span>ğŸ“… {article.date}</span>
                        <span>â±ï¸ {article.read_time}</span>
                        {f'<span>ğŸ“ {article.location}</span>' if article.location else ''}
                    </div>
                </div>
            </div>'''


def update_index_html(article: ArticleMetadata, target_dir: str, section_id: str):
    """æ›´æ–°index.htmlä¸­çš„æ–‡ç« åˆ—è¡¨ - ä¸»é¡µæ¯ä¸ªæ ç›®æœ€å¤šæ˜¾ç¤º5ç¯‡æ–‡ç« """
    if not INDEX_HTML.exists():
        print(f"âš ï¸  {INDEX_HTML} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    with open(INDEX_HTML, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾å¯¹åº”çš„section
    section = soup.find('section', id=section_id)
    if not section:
        print(f"âš ï¸  æœªæ‰¾åˆ°section #{section_id}")
        return
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
    article_list = section.find(class_='article-list')
    if not article_list:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
        return
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹å¹¶ä¿å­˜å®ƒä»¬çš„HTMLå­—ç¬¦ä¸²ï¼ˆåœ¨æ¸…é™¤ä¹‹å‰ï¼‰
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    existing_items_html = [str(item) for item in existing_items]
    
    # ç”Ÿæˆæ–°æ–‡ç« é¡¹ï¼ˆä»index.htmlè°ƒç”¨ï¼Œéœ€è¦å®Œæ•´è·¯å¾„ï¼‰
    article_html = generate_article_list_item(article, target_dir, from_index='root')
    
    # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
    article_list.clear()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    new_item = BeautifulSoup(article_html, 'html.parser')
    article_list.append(new_item)
    
    # é‡æ–°æ’å…¥ç°æœ‰æ–‡ç« ï¼ˆé™åˆ¶æ•°é‡ï¼šä¸»é¡µæœ€å¤šæ˜¾ç¤º5ç¯‡ï¼‰
    max_items = MAX_HOMEPAGE_STORIES if section_id == 'stories' else MAX_HOMEPAGE_GUIDES
    for item_html in existing_items_html[:max_items - 1]:
        existing_item = BeautifulSoup(item_html, 'html.parser')
        article_list.append(existing_item)
    
    # ä¿å­˜
    with open(INDEX_HTML, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²æ›´æ–° {INDEX_HTML} çš„ {section_id} éƒ¨åˆ†ï¼ˆæ˜¾ç¤ºæœ€æ–° {max_items} ç¯‡ï¼‰")


def update_blog_index(article: ArticleMetadata):
    """æ›´æ–°blog/index.html - å­é¡µé¢å¿…é¡»æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« """
    if not BLOG_INDEX.exists():
        print(f"âš ï¸  {BLOG_INDEX} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    with open(BLOG_INDEX, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
    article_list = soup.find(class_='article-list')
    if not article_list:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
        return
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹å¹¶ä¿å­˜å®ƒä»¬çš„HTMLå­—ç¬¦ä¸²ï¼ˆåœ¨æ¸…é™¤ä¹‹å‰ï¼‰
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    existing_items_html = [str(item) for item in existing_items]
    
    # ç”Ÿæˆæ–°æ–‡ç« é¡¹ï¼ˆä»blog/index.htmlè°ƒç”¨ï¼Œåªéœ€è¦æ–‡ä»¶åï¼‰
    article_html = generate_article_list_item(article, 'blog', from_index='blog')
    
    # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
    article_list.clear()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    new_item = BeautifulSoup(article_html, 'html.parser')
    article_list.append(new_item)
    
    # é‡æ–°æ’å…¥æ‰€æœ‰ç°æœ‰æ–‡ç« ï¼ˆå­é¡µé¢æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰
    for item_html in existing_items_html:
        existing_item = BeautifulSoup(item_html, 'html.parser')
        article_list.append(existing_item)
    
    # ä¿å­˜
    with open(BLOG_INDEX, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²æ›´æ–° {BLOG_INDEX}ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼‰")


def update_guides_index(article: ArticleMetadata):
    """æ›´æ–°guides/index.html - å­é¡µé¢å¿…é¡»æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« """
    if not GUIDES_INDEX.exists():
        print(f"âš ï¸  {GUIDES_INDEX} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    with open(GUIDES_INDEX, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ ¹æ®æ–‡ç« å†…å®¹åˆ¤æ–­åº”è¯¥æ”¾åœ¨å“ªä¸ªsection
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ”¾åœ¨ç¬¬ä¸€ä¸ªåˆé€‚çš„section
    sections = soup.find_all('section')
    target_section = None
    
    title_lower = article.title.lower()
    if 'visa' in title_lower:
        target_section = soup.find('section', id='visa')
    elif any(word in title_lower for word in ['train', 'rail', 'transport', 'metro', 'didi']):
        target_section = soup.find('section', id='transport')
    elif any(word in title_lower for word in ['app', 'vpn', 'internet', 'wechat', 'alipay']):
        target_section = soup.find('section', id='tech')
    elif any(word in title_lower for word in ['food', 'dining', 'restaurant', 'ordering']):
        target_section = soup.find('section', id='food')
    elif any(word in title_lower for word in ['city', 'beijing', 'shanghai', 'chengdu']):
        target_section = soup.find('section', id='cities')
    
    # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šsectionï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªsection
    if not target_section and sections:
        target_section = sections[0]
    
    if not target_section:
        print(f"âš ï¸  æœªæ‰¾åˆ°åˆé€‚çš„section")
        return
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
    article_list = target_section.find(class_='article-list')
    if not article_list:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
        return
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹å¹¶ä¿å­˜å®ƒä»¬çš„HTMLå­—ç¬¦ä¸²ï¼ˆåœ¨æ¸…é™¤ä¹‹å‰ï¼‰
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    existing_items_html = [str(item) for item in existing_items]
    
    # ç”Ÿæˆæ–°æ–‡ç« é¡¹ï¼ˆä»guides/index.htmlè°ƒç”¨ï¼Œåªéœ€è¦æ–‡ä»¶åï¼‰
    article_html = generate_article_list_item(article, 'guides', from_index='guides')
    
    # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
    article_list.clear()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    new_item = BeautifulSoup(article_html, 'html.parser')
    article_list.append(new_item)
    
    # é‡æ–°æ’å…¥æ‰€æœ‰ç°æœ‰æ–‡ç« ï¼ˆå­é¡µé¢æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰
    for item_html in existing_items_html:
        existing_item = BeautifulSoup(item_html, 'html.parser')
        article_list.append(existing_item)
    
    # ä¿å­˜
    with open(GUIDES_INDEX, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²æ›´æ–° {GUIDES_INDEX}ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼‰")


def fix_article_paths(html_file: Path, target_dir: str):
    """ä¿®å¤æ–‡ç« ä¸­çš„ç›¸å¯¹è·¯å¾„"""
    with open(html_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    soup = BeautifulSoup(content, 'html.parser')
    
    # ä¿®å¤å¯¼èˆªé“¾æ¥
    # blogå’Œguidesç›®å½•ä¸­çš„æ–‡ç« éƒ½éœ€è¦../è¿”å›æ ¹ç›®å½•
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        if not href or href.startswith('http') or href.startswith('#'):
            continue  # è·³è¿‡å¤–éƒ¨é“¾æ¥å’Œé”šç‚¹
        
        # ä¿®å¤index.htmlé“¾æ¥
        if href == 'index.html' or href.endswith('/index.html'):
            link['href'] = '../index.html'
        # ä¿®å¤blog/index.htmlå’Œguides/index.htmlé“¾æ¥
        elif href == 'blog/index.html':
            link['href'] = '../blog/index.html'
        elif href == 'guides/index.html':
            link['href'] = '../guides/index.html'
        # ä¿®å¤å…¶ä»–ç›¸å¯¹è·¯å¾„
        elif href.startswith('blog/') and not href.startswith('../blog/'):
            link['href'] = '../' + href
        elif href.startswith('guides/') and not href.startswith('../guides/'):
            link['href'] = '../' + href
    
    # ä¿®å¤logoé“¾æ¥
    logo = soup.find(class_='logo')
    if logo and logo.get('href'):
        if logo['href'] == 'index.html':
            logo['href'] = '../index.html'
    
    # ä¿å­˜
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²ä¿®å¤è·¯å¾„: {html_file.name}")


def deploy_article(source_file: Path, target_dir: str, auto_detect: bool = False) -> bool:
    """éƒ¨ç½²å•ç¯‡æ–‡ç« """
    try:
        # æå–å…ƒæ•°æ®
        article = ArticleMetadata(source_file)
        if not article.extract_from_html():
            return False
        
        # è‡ªåŠ¨åˆ¤æ–­ç›®æ ‡ç›®å½•
        if auto_detect:
            target_dir = article.determine_category()
        
        # ç¡®å®šç›®æ ‡ç›®å½•
        if target_dir == 'blog':
            target_path = BLOG_DIR
            section_id = 'stories'
        elif target_dir == 'guides':
            target_path = GUIDES_DIR
            section_id = 'guides'
        else:
            print(f"âŒ æ— æ•ˆçš„ç›®æ ‡ç›®å½•: {target_dir}")
            return False
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        target_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç›®æ ‡æ–‡ä»¶å
        target_filename = safe_filename(article.title) + '.html'
        target_file = target_path / target_filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯¢é—®æ˜¯å¦è¦†ç›–
        if target_file.exists():
            print(f"âš ï¸  æ–‡ä»¶å·²å­˜åœ¨: {target_file}")
            response = input("æ˜¯å¦è¦†ç›–? (y/n): ").strip().lower()
            if response != 'y':
                print("â­ï¸  è·³è¿‡æ­¤æ–‡ä»¶")
                return False
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(source_file, target_file)
        print(f"âœ… å·²å¤åˆ¶æ–‡ä»¶: {target_file}")
        
        # ä¿®å¤æ–‡ç« ä¸­çš„è·¯å¾„
        fix_article_paths(target_file, target_dir)
        
        # æ›´æ–°ç´¢å¼•é¡µ
        if target_dir == 'blog':
            update_blog_index(article)
        else:
            update_guides_index(article)
        
        # æ›´æ–°ä¸»é¡µ
        update_index_html(article, target_dir, section_id)
        
        print(f"âœ… æˆåŠŸéƒ¨ç½²: {article.title}")
        return True
        
    except Exception as e:
        print(f"âŒ éƒ¨ç½²å¤±è´¥ {source_file}: {e}")
        import traceback
        traceback.print_exc()
        return False


def rebuild_index(target_dir: str):
    """ä»æ–‡ä»¶ç³»ç»Ÿæ‰«ææ‰€æœ‰æ–‡ç« å¹¶é‡å»ºç´¢å¼•"""
    if target_dir == 'blog':
        target_path = BLOG_DIR
        index_file = BLOG_INDEX
    elif target_dir == 'guides':
        target_path = GUIDES_DIR
        index_file = GUIDES_INDEX
    else:
        print(f"âŒ æ— æ•ˆçš„ç›®æ ‡ç›®å½•: {target_dir}")
        return
    
    if not target_path.exists():
        print(f"âŒ ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_path}")
        return
    
    if not index_file.exists():
        print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
        return
    
    # æ‰«ææ‰€æœ‰HTMLæ–‡ä»¶ï¼ˆæ’é™¤index.htmlï¼‰
    article_files = [f for f in target_path.glob('*.html') if f.name != 'index.html']
    
    if not article_files:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« æ–‡ä»¶: {target_path}")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(article_files)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹é‡å»ºç´¢å¼•...")
    
    # æå–æ‰€æœ‰æ–‡ç« çš„å…ƒæ•°æ®
    articles = []
    for article_file in article_files:
        article = ArticleMetadata(article_file)
        if article.extract_from_html():
            articles.append(article)
            print(f"  âœ“ {article.title}")
        else:
            print(f"  âœ— è·³è¿‡ {article_file.name}ï¼ˆæ— æ³•æå–å…ƒæ•°æ®ï¼‰")
    
    if not articles:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ç« å¯ä»¥æ·»åŠ åˆ°ç´¢å¼•")
        return
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    articles.sort(key=lambda x: x.date, reverse=True)
    
    # è¯»å–ç´¢å¼•æ–‡ä»¶
    with open(index_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    if target_dir == 'blog':
        # blog/index.html åªæœ‰ä¸€ä¸ª article-list
        article_list = soup.find(class_='article-list')
        if article_list:
            # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
            article_list.clear()
            
            # æ·»åŠ æ‰€æœ‰æ–‡ç« 
            for article in articles:
                article_html = generate_article_list_item(article, 'blog', from_index='blog')
                article_item = BeautifulSoup(article_html, 'html.parser')
                article_list.append(article_item)
            
            print(f"âœ… å·²é‡å»º {index_file}ï¼ŒåŒ…å« {len(articles)} ç¯‡æ–‡ç« ")
    else:
        # guides/index.html æœ‰å¤šä¸ª sectionï¼Œæ¯ä¸ª section æœ‰è‡ªå·±çš„ article-list
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå°†æ‰€æœ‰æ–‡ç« æ·»åŠ åˆ°ç¬¬ä¸€ä¸ªåˆé€‚çš„ section
        sections = soup.find_all('section')
        if sections:
            # ä¸ºæ¯ç¯‡æ–‡ç« æ‰¾åˆ°åˆé€‚çš„ section
            for article in articles:
                title_lower = article.title.lower()
                target_section = None
                
                if 'visa' in title_lower:
                    target_section = soup.find('section', id='visa')
                elif any(word in title_lower for word in ['train', 'rail', 'transport', 'metro', 'didi']):
                    target_section = soup.find('section', id='transport')
                elif any(word in title_lower for word in ['app', 'vpn', 'internet', 'wechat', 'alipay']):
                    target_section = soup.find('section', id='tech')
                elif any(word in title_lower for word in ['food', 'dining', 'restaurant', 'ordering']):
                    target_section = soup.find('section', id='food')
                elif any(word in title_lower for word in ['city', 'beijing', 'shanghai', 'chengdu']):
                    target_section = soup.find('section', id='cities')
                
                if not target_section and sections:
                    target_section = sections[0]
                
                if target_section:
                    article_list = target_section.find(class_='article-list')
                    if article_list:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
                        existing_titles = [item.find('h3').get_text() if item.find('h3') else '' 
                                          for item in article_list.find_all(class_='article-list-item', recursive=False)]
                        if article.title not in existing_titles:
                            article_html = generate_article_list_item(article, 'guides', from_index='guides')
                            article_item = BeautifulSoup(article_html, 'html.parser')
                            article_list.append(article_item)
            
            print(f"âœ… å·²é‡å»º {index_file}ï¼ŒåŒ…å« {len(articles)} ç¯‡æ–‡ç« ")
    
    # ä¿å­˜
    with open(index_file, 'w', encoding='utf-8') as f:
        f.write(str(soup))


def rebuild_homepage():
    """é‡å»ºä¸»é¡µé¢çš„ç´¢å¼•ï¼ˆä»blogå’Œguidesç›®å½•æ‰«ææœ€æ–°æ–‡ç« ï¼‰"""
    if not INDEX_HTML.exists():
        print(f"âŒ ä¸»é¡µé¢ä¸å­˜åœ¨: {INDEX_HTML}")
        return
    
    print("ğŸ”„ å¼€å§‹é‡å»ºä¸»é¡µé¢ç´¢å¼•...")
    
    # æ‰«æ blog ç›®å½•çš„æ–‡ç« 
    blog_articles = []
    if BLOG_DIR.exists():
        blog_files = [f for f in BLOG_DIR.glob('*.html') if f.name != 'index.html']
        print(f"\nğŸ“ æ‰«æ blog ç›®å½•ï¼Œæ‰¾åˆ° {len(blog_files)} ç¯‡æ–‡ç« ")
        for article_file in blog_files:
            article = ArticleMetadata(article_file)
            if article.extract_from_html():
                blog_articles.append(article)
    
    # æ‰«æ guides ç›®å½•çš„æ–‡ç« 
    guides_articles = []
    if GUIDES_DIR.exists():
        guides_files = [f for f in GUIDES_DIR.glob('*.html') if f.name != 'index.html']
        print(f"ğŸ“ æ‰«æ guides ç›®å½•ï¼Œæ‰¾åˆ° {len(guides_files)} ç¯‡æ–‡ç« ")
        for article_file in guides_files:
            article = ArticleMetadata(article_file)
            if article.extract_from_html():
                guides_articles.append(article)
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    blog_articles.sort(key=lambda x: x.date, reverse=True)
    guides_articles.sort(key=lambda x: x.date, reverse=True)
    
    # è¯»å–ä¸»é¡µé¢
    with open(INDEX_HTML, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ›´æ–° stories section
    stories_section = soup.find('section', id='stories')
    if stories_section:
        article_list = stories_section.find(class_='article-list')
        if article_list:
            # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
            article_list.clear()
            
            # æ·»åŠ æœ€æ–°çš„5ç¯‡ blog æ–‡ç« 
            for article in blog_articles[:MAX_HOMEPAGE_STORIES]:
                article_html = generate_article_list_item(article, 'blog', from_index='root')
                article_item = BeautifulSoup(article_html, 'html.parser')
                article_list.append(article_item)
            
            print(f"âœ… å·²æ›´æ–° stories sectionï¼Œæ˜¾ç¤º {min(len(blog_articles), MAX_HOMEPAGE_STORIES)} ç¯‡æ–‡ç« ")
    
    # æ›´æ–° guides section
    guides_section = soup.find('section', id='guides')
    if guides_section:
        article_list = guides_section.find(class_='article-list')
        if article_list:
            # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
            article_list.clear()
            
            # æ·»åŠ æœ€æ–°çš„5ç¯‡ guides æ–‡ç« 
            for article in guides_articles[:MAX_HOMEPAGE_GUIDES]:
                article_html = generate_article_list_item(article, 'guides', from_index='root')
                article_item = BeautifulSoup(article_html, 'html.parser')
                article_list.append(article_item)
            
            print(f"âœ… å·²æ›´æ–° guides sectionï¼Œæ˜¾ç¤º {min(len(guides_articles), MAX_HOMEPAGE_GUIDES)} ç¯‡æ–‡ç« ")
    
    # ä¿å­˜
    with open(INDEX_HTML, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"\nâœ… å·²é‡å»ºä¸»é¡µé¢ç´¢å¼•")


def deploy_all(source_dir: Path, target_dir: str = None, auto_detect: bool = False):
    """éƒ¨ç½²æ‰€æœ‰æ–‡ç« """
    if not source_dir.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶
    html_files = list(source_dir.glob('*.html'))
    if not html_files:
        print(f"âš ï¸  æœªæ‰¾åˆ°HTMLæ–‡ä»¶: {source_dir}")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    success_count = 0
    for html_file in html_files:
        print(f"\nğŸ“„ å¤„ç†: {html_file.name}")
        if deploy_article(html_file, target_dir or 'blog', auto_detect):
            success_count += 1
    
    print(f"\n{'='*50}")
    print(f"âœ… æˆåŠŸéƒ¨ç½²: {success_count}/{len(html_files)}")
    print(f"{'='*50}")


def remove_article_from_index(index_file: Path, article_filename: str, article_title: str = None):
    """ä»ç´¢å¼•é¡µä¸­ç§»é™¤æŒ‡å®šæ–‡ç« 
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        article_filename: æ–‡ç« æ–‡ä»¶åï¼ˆç”¨äºåŒ¹é…é“¾æ¥ï¼‰
        article_title: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´ç²¾ç¡®åŒ¹é…ï¼‰
    """
    if not index_file.exists():
        return False
    
    try:
        with open(index_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        removed = False
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« åˆ—è¡¨ï¼ˆå¯èƒ½å¤šä¸ªsectionï¼‰
        article_lists = soup.find_all(class_='article-list')
        
        for article_list in article_lists:
            items = article_list.find_all(class_='article-list-item', recursive=False)
            for item in items:
                # æ£€æŸ¥onclickå±æ€§ï¼ˆæ–‡ç« é¡¹ä½¿ç”¨onclickè·³è½¬ï¼‰
                onclick = item.get('onclick', '') if hasattr(item, 'get') else ''
                
                # åŒ¹é…æ–‡ä»¶åï¼ˆonclickæ ¼å¼: window.location.href='filename.html' æˆ– 'filename.html'ï¼‰
                if article_filename in onclick:
                    item.decompose()
                    removed = True
                    continue
                
                # æ£€æŸ¥å†…éƒ¨é“¾æ¥
                link = item.find('a')
                if link:
                    href = link.get('href', '')
                    if article_filename in href:
                        item.decompose()
                        removed = True
                        continue
                
                # å¦‚æœæä¾›äº†æ ‡é¢˜ï¼Œä¹Ÿé€šè¿‡æ ‡é¢˜åŒ¹é…
                if article_title:
                    h3 = item.find('h3')
                    if h3:
                        item_title = h3.get_text().strip()
                        # ç²¾ç¡®åŒ¹é…æˆ–åŒ…å«åŒ¹é…
                        if article_title == item_title or article_title in item_title:
                            item.decompose()
                            removed = True
                            continue
        
        if removed:
            with open(index_file, 'w', encoding='utf-8') as f:
                f.write(str(soup))
            return True
        
        return False
    except Exception as e:
        print(f"âš ï¸  ä» {index_file} ç§»é™¤æ–‡ç« æ—¶å‡ºé”™: {e}")
        return False


def remove_article_from_homepage(article_filename: str, target_dir: str, article_title: str = None):
    """ä»ä¸»é¡µä¸­ç§»é™¤æŒ‡å®šæ–‡ç« """
    if not INDEX_HTML.exists():
        return False
    
    try:
        with open(INDEX_HTML, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        removed = False
        
        # ç¡®å®šè¦æ£€æŸ¥çš„section
        section_id = 'stories' if target_dir == 'blog' else 'guides'
        section = soup.find('section', id=section_id)
        
        if section:
            article_list = section.find(class_='article-list')
            if article_list:
                items = article_list.find_all(class_='article-list-item', recursive=False)
                for item in items:
                    # æ£€æŸ¥onclickå±æ€§ï¼ˆä¸»é¡µä½¿ç”¨å®Œæ•´è·¯å¾„ blog/filename.html æˆ– guides/filename.htmlï¼‰
                    onclick = item.get('onclick', '') if hasattr(item, 'get') else ''
                    
                    # åŒ¹é…å®Œæ•´è·¯å¾„
                    full_path = f"{target_dir}/{article_filename}"
                    if full_path in onclick:
                        item.decompose()
                        removed = True
                        continue
                    
                    # æ£€æŸ¥å†…éƒ¨é“¾æ¥
                    link = item.find('a')
                    if link:
                        href = link.get('href', '')
                        if full_path in href:
                            item.decompose()
                            removed = True
                            continue
                    
                    # å¦‚æœæä¾›äº†æ ‡é¢˜ï¼Œä¹Ÿé€šè¿‡æ ‡é¢˜åŒ¹é…
                    if article_title:
                        h3 = item.find('h3')
                        if h3:
                            item_title = h3.get_text().strip()
                            # ç²¾ç¡®åŒ¹é…æˆ–åŒ…å«åŒ¹é…
                            if article_title == item_title or article_title in item_title:
                                item.decompose()
                                removed = True
                                continue
        
        if removed:
            with open(INDEX_HTML, 'w', encoding='utf-8') as f:
                f.write(str(soup))
            return True
        
        return False
    except Exception as e:
        print(f"âš ï¸  ä»ä¸»é¡µç§»é™¤æ–‡ç« æ—¶å‡ºé”™: {e}")
        return False


def delete_article(article_path: str, target_dir: str = None) -> bool:
    """åˆ é™¤å·²éƒ¨ç½²çš„æ–‡ç« 
    
    Args:
        article_path: æ–‡ç« æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å®Œæ•´è·¯å¾„æˆ–æ–‡ä»¶åï¼‰
        target_dir: ç›®æ ‡ç›®å½•ï¼ˆblog/guidesï¼‰ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™è‡ªåŠ¨æ£€æµ‹
    """
    try:
        # è§£ææ–‡ä»¶è·¯å¾„
        article_file = Path(article_path)
        
        # å¦‚æœè·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ä¸”ä¸åŒ…å«ç›®å½•ï¼Œå°è¯•åœ¨blogå’Œguidesä¸­æŸ¥æ‰¾
        if not article_file.is_absolute() and '/' not in article_path and '\\' not in article_path:
            if target_dir:
                # å¦‚æœæŒ‡å®šäº†ç›®æ ‡ç›®å½•ï¼Œåœ¨è¯¥ç›®å½•ä¸­æŸ¥æ‰¾
                if target_dir == 'blog':
                    article_file = BLOG_DIR / article_path
                elif target_dir == 'guides':
                    article_file = GUIDES_DIR / article_path
                else:
                    print(f"âŒ æ— æ•ˆçš„ç›®æ ‡ç›®å½•: {target_dir}")
                    return False
            else:
                # è‡ªåŠ¨æ£€æµ‹ï¼šå…ˆåœ¨blogä¸­æŸ¥æ‰¾ï¼Œå†åœ¨guidesä¸­æŸ¥æ‰¾
                blog_file = BLOG_DIR / article_path
                guides_file = GUIDES_DIR / article_path
                
                if blog_file.exists() and guides_file.exists():
                    print(f"âš ï¸  åœ¨blogå’Œguidesç›®å½•ä¸­éƒ½æ‰¾åˆ°äº†æ–‡ä»¶: {article_path}")
                    print("è¯·ä½¿ç”¨ --target å‚æ•°æŒ‡å®šç›®å½•ï¼Œæˆ–ä½¿ç”¨å®Œæ•´è·¯å¾„")
                    return False
                elif blog_file.exists():
                    article_file = blog_file
                    target_dir = 'blog'
                elif guides_file.exists():
                    article_file = guides_file
                    target_dir = 'guides'
                else:
                    print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {article_path}")
                    print(f"   åœ¨ {BLOG_DIR} å’Œ {GUIDES_DIR} ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°")
                    return False
        else:
            # å®Œæ•´è·¯å¾„ï¼Œç¡®å®šç›®æ ‡ç›®å½•
            article_file = Path(article_path)
            if not article_file.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {article_file}")
                return False
            
            # ä»è·¯å¾„åˆ¤æ–­ç›®æ ‡ç›®å½•
            if not target_dir:
                if 'blog' in str(article_file):
                    target_dir = 'blog'
                elif 'guides' in str(article_file):
                    target_dir = 'guides'
                else:
                    print(f"âš ï¸  æ— æ³•ä»è·¯å¾„åˆ¤æ–­ç›®æ ‡ç›®å½•ï¼Œè¯·ä½¿ç”¨ --target å‚æ•°")
                    return False
        
        # ç¡®è®¤æ–‡ä»¶å­˜åœ¨
        if not article_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {article_file}")
            return False
        
        # æå–æ–‡ç« ä¿¡æ¯ï¼ˆç”¨äºä»ç´¢å¼•ä¸­ç§»é™¤ï¼‰
        article = ArticleMetadata(article_file)
        article.extract_from_html()  # å°è¯•æå–ï¼Œå¤±è´¥ä¹Ÿä¸å½±å“åˆ é™¤
        
        article_filename = article_file.name
        article_title = article.title if article.title else None
        
        # ç¡®è®¤åˆ é™¤
        print(f"ğŸ“„ å‡†å¤‡åˆ é™¤æ–‡ç« : {article_file}")
        if article_title:
            print(f"   æ ‡é¢˜: {article_title}")
        print(f"   ç›®å½•: {target_dir}")
        
        response = input("ç¡®è®¤åˆ é™¤? (y/n): ").strip().lower()
        if response != 'y':
            print("â­ï¸  å–æ¶ˆåˆ é™¤")
            return False
        
        # ä»ç´¢å¼•é¡µä¸­ç§»é™¤
        if target_dir == 'blog':
            if remove_article_from_index(BLOG_INDEX, article_filename, article_title):
                print(f"âœ… å·²ä» {BLOG_INDEX} ä¸­ç§»é™¤")
        else:
            if remove_article_from_index(GUIDES_INDEX, article_filename, article_title):
                print(f"âœ… å·²ä» {GUIDES_INDEX} ä¸­ç§»é™¤")
        
        # ä»ä¸»é¡µä¸­ç§»é™¤
        if remove_article_from_homepage(article_filename, target_dir, article_title):
            print(f"âœ… å·²ä»ä¸»é¡µä¸­ç§»é™¤")
        
        # åˆ é™¤æ–‡ä»¶
        article_file.unlink()
        print(f"âœ… å·²åˆ é™¤æ–‡ä»¶: {article_file}")
        
        print(f"âœ… æˆåŠŸåˆ é™¤æ–‡ç« : {article_title or article_filename}")
        return True
        
    except Exception as e:
        print(f"âŒ åˆ é™¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨éƒ¨ç½²ç¿»è¯‘åçš„æ–‡ç« ')
    parser.add_argument('--source-dir', '-s', 
                       default=str(TRANSLATED_DIR),
                       help='æºç›®å½•ï¼ˆåŒ…å«ç¿»è¯‘åçš„HTMLæ–‡ä»¶ï¼‰')
    parser.add_argument('--target', '-t', 
                       choices=['blog', 'guides'],
                       help='ç›®æ ‡ç›®å½•ï¼ˆblogæˆ–guidesï¼‰')
    parser.add_argument('--auto', '-a', 
                       action='store_true',
                       help='è‡ªåŠ¨åˆ¤æ–­æ˜¯blogè¿˜æ˜¯guides')
    parser.add_argument('--file', '-f',
                       help='éƒ¨ç½²å•ä¸ªæ–‡ä»¶ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªç›®å½•ï¼‰')
    parser.add_argument('--rebuild', '-r',
                       choices=['blog', 'guides', 'homepage'],
                       help='ä»æ–‡ä»¶ç³»ç»Ÿé‡å»ºç´¢å¼•ï¼ˆblogã€guidesæˆ–homepageï¼‰')
    parser.add_argument('--delete', '-d',
                       help='åˆ é™¤å·²éƒ¨ç½²çš„æ–‡ç« ï¼ˆæŒ‡å®šæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶åï¼‰')
    
    args = parser.parse_args()
    
    source_dir = Path(args.source_dir)
    
    # å¦‚æœæŒ‡å®šäº†åˆ é™¤
    if args.delete:
        delete_article(args.delete, args.target)
        return
    
    # å¦‚æœæŒ‡å®šäº†é‡å»ºç´¢å¼•
    if args.rebuild:
        if args.rebuild == 'homepage':
            rebuild_homepage()
        else:
            rebuild_index(args.rebuild)
        return
    
    # å¦‚æœæŒ‡å®šäº†å•ä¸ªæ–‡ä»¶
    if args.file:
        source_file = Path(args.file)
        if not source_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
            sys.exit(1)
        
        target_dir = args.target or 'blog'
        auto_detect = args.auto
        deploy_article(source_file, target_dir, auto_detect)
    else:
        # éƒ¨ç½²æ•´ä¸ªç›®å½•
        if args.auto:
            deploy_all(source_dir, auto_detect=True)
        elif args.target:
            deploy_all(source_dir, target_dir=args.target)
        else:
            print("âŒ è¯·æŒ‡å®š --target (blog/guides) æˆ–ä½¿ç”¨ --auto è‡ªåŠ¨åˆ¤æ–­")
            sys.exit(1)


if __name__ == '__main__':
    main()