#!/usr/bin/env python3
"""
è‡ªåŠ¨éƒ¨ç½²è„šæœ¬ï¼šå°†ç¿»è¯‘åçš„æ–‡ç« éƒ¨ç½²åˆ°blogæˆ–guidesç›®å½•ï¼Œå¹¶æ›´æ–°ä¸»é¡µå’Œç´¢å¼•é¡µ

åŠŸèƒ½ï¼š
1. æ‰«ætranslated_articlesç›®å½•ä¸­çš„HTMLæ–‡ä»¶
2. æå–æ–‡ç« å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€æè¿°ã€æ—¥æœŸç­‰ï¼‰
3. æ ¹æ®é…ç½®æˆ–å†…å®¹åˆ¤æ–­æ˜¯blogè¿˜æ˜¯guides
4. å°†æ–‡ä»¶ç§»åŠ¨åˆ°å¯¹åº”ç›®å½•
5. æ›´æ–°index.htmlã€blog/index.htmlå’Œguides/index.htmlä¸­çš„æ–‡ç« åˆ—è¡¨
6. ä¿æŒæœ€æ–°çš„æ–‡ç« åœ¨ä¸»é¡µæ˜¾ç¤º

ç”¨æ³•ï¼š
    # æ–¹å¼1ï¼šè‡ªåŠ¨åˆ¤æ–­blogæˆ–guidesï¼ˆæ¨èï¼‰
    python3 mcp/deploy.py --auto

    # æ–¹å¼2ï¼šæŒ‡å®šéƒ¨ç½²åˆ°blogç›®å½•
    python3 mcp/deploy.py --target blog

    # æ–¹å¼3ï¼šæŒ‡å®šéƒ¨ç½²åˆ°guidesç›®å½•
    python3 mcp/deploy.py --target guides

    # æ–¹å¼4ï¼šéƒ¨ç½²å•ä¸ªæ–‡ä»¶
    python3 mcp/deploy.py --file translated_articles/article.html --target blog

    # æ–¹å¼5ï¼šæŒ‡å®šæºç›®å½•
    python3 mcp/deploy.py --source-dir mcp/translated_articles --auto
"""

import os
import re
import sys
import argparse
import shutil
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Tuple
import json

# é…ç½®
BLOG_DIR = Path(__file__).parent.parent / 'blog'
GUIDES_DIR = Path(__file__).parent.parent / 'guides'
INDEX_HTML = Path(__file__).parent.parent / 'index.html'
BLOG_INDEX = BLOG_DIR / 'index.html'
GUIDES_INDEX = GUIDES_DIR / 'index.html'
TRANSLATED_DIR = Path(__file__).parent / 'translated_articles'

# Blogå’ŒGuidesçš„å…³é”®è¯ï¼ˆç”¨äºè‡ªåŠ¨åˆ†ç±»ï¼‰
BLOG_KEYWORDS = ['story', 'experience', 'adventure', 'journey', 'trip', 'travel', 'personal', 
                 'narrative', 'tale', 'moment', 'encounter', 'memory', 'å›å¿†', 'æ•…äº‹', 'ç»å†']
GUIDES_KEYWORDS = ['guide', 'how to', 'tutorial', 'tips', 'advice', 'information', 'complete',
                   'essential', 'visa', 'transport', 'app', 'vpn', 'food', 'ordering',
                   'æ”»ç•¥', 'æŒ‡å—', 'å¦‚ä½•', 'æ•™ç¨‹', 'ä¿¡æ¯']

# ä¸»é¡µæ˜¾ç¤ºçš„æ–‡ç« æ•°é‡
MAX_HOMEPAGE_STORIES = 5
MAX_HOMEPAGE_GUIDES = 5


class ArticleMetadata:
    """æ–‡ç« å…ƒæ•°æ®"""
    def __init__(self, file_path: Path):
        self.file_path = file_path
        self.title = ""
        self.description = ""
        self.date = datetime.now().strftime('%B %d, %Y')
        self.read_time = "10 min read"
        self.location = ""
        self.category = "TRAVEL STORY"
        self.icon = "ğŸ“–"
        self.content_preview = ""
        self.source_url = ""
        
    def extract_from_html(self) -> bool:
        """ä»HTMLæ–‡ä»¶ä¸­æå–å…ƒæ•°æ®"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_tag = soup.find('title')
            if title_tag:
                self.title = title_tag.get_text().replace(' - Travel-China.Help', '').strip()
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                self.title = re.sub(r'\*\*', '', self.title)  # ç§»é™¤markdownæ ¼å¼
            
            # ä»article-titleç±»ä¸­æå–æ ‡é¢˜ï¼ˆæ›´å‡†ç¡®ï¼‰
            article_title = soup.find(class_='article-title')
            if article_title:
                self.title = article_title.get_text().strip()
            
            # æå–æè¿°
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                self.description = meta_desc.get('content').strip()
            
            # æå–æ—¥æœŸ
            meta_date = soup.find(class_='meta-item')
            if meta_date:
                date_text = meta_date.get_text()
                # å°è¯•æå–æ—¥æœŸ
                date_match = re.search(r'(\w+ \d{1,2}, \d{4})', date_text)
                if date_match:
                    self.date = date_match.group(1)
            
            # æå–å†…å®¹é¢„è§ˆï¼ˆå‰200ä¸ªå­—ç¬¦ï¼‰
            content_div = soup.find(class_='article-content')
            if content_div:
                text = content_div.get_text(strip=True)
                self.content_preview = text[:200] + '...' if len(text) > 200 else text
                # ä¼°ç®—é˜…è¯»æ—¶é—´ï¼ˆå‡è®¾æ¯åˆ†é’Ÿ200å­—ï¼‰
                word_count = len(text.split())
                self.read_time = f"{max(3, word_count // 200)} min read"
            
            # æå–æºURL
            source_info = soup.find(class_='source-info')
            if source_info:
                source_link = source_info.find('a')
                if source_link:
                    self.source_url = source_link.get('href', '')
            
            # å¦‚æœæ²¡æœ‰æè¿°ï¼Œä½¿ç”¨å†…å®¹é¢„è§ˆ
            if not self.description:
                self.description = self.content_preview[:150] + '...' if len(self.content_preview) > 150 else self.content_preview
            
            return True
        except Exception as e:
            print(f"âŒ æå–å…ƒæ•°æ®å¤±è´¥ {self.file_path}: {e}")
            return False
    
    def determine_category(self) -> str:
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹åˆ¤æ–­æ˜¯blogè¿˜æ˜¯guides"""
        title_lower = self.title.lower()
        desc_lower = self.description.lower()
        content_lower = self.content_preview.lower()
        
        # æ£€æŸ¥guideså…³é”®è¯
        for keyword in GUIDES_KEYWORDS:
            if keyword in title_lower or keyword in desc_lower or keyword in content_lower:
                return 'guides'
        
        # æ£€æŸ¥blogå…³é”®è¯
        for keyword in BLOG_KEYWORDS:
            if keyword in title_lower or keyword in desc_lower or keyword in content_lower:
                return 'blog'
        
        # é»˜è®¤ï¼šå¦‚æœåŒ…å«"guide"ã€"how"ã€"tutorial"ç­‰ï¼Œå½’ä¸ºguides
        if any(word in title_lower for word in ['guide', 'how to', 'tutorial', 'tips', 'visa', 'app']):
            return 'guides'
        
        # å¦åˆ™å½’ä¸ºblog
        return 'blog'
    
    def get_icon(self) -> str:
        """æ ¹æ®æ ‡é¢˜å†…å®¹é€‰æ‹©åˆé€‚çš„å›¾æ ‡"""
        title_lower = self.title.lower()
        
        icon_map = {
            'visa': 'ğŸ›‚', 'passport': 'ğŸ›‚',
            'train': 'ğŸš„', 'rail': 'ğŸš„', 'transport': 'ğŸš„',
            'app': 'ğŸ“±', 'wechat': 'ğŸ“±', 'alipay': 'ğŸ“±',
            'vpn': 'ğŸŒ', 'internet': 'ğŸŒ',
            'food': 'ğŸœ', 'restaurant': 'ğŸœ', 'dining': 'ğŸœ', 'hotpot': 'ğŸŒ¶ï¸',
            'mountain': 'â›°ï¸', 'hiking': 'ğŸ¥¾', 'climb': 'â›°ï¸',
            'city': 'ğŸ›ï¸', 'beijing': 'ğŸ›ï¸', 'shanghai': 'ğŸŒƒ', 'chengdu': 'ğŸ¼',
            'story': 'ğŸ“–', 'experience': 'â¤ï¸', 'adventure': 'ğŸ¥¾',
            'funny': 'ğŸ˜‚', 'humor': 'ğŸ˜‚',
            'culture': 'ğŸ­', 'cultural': 'ğŸ­',
        }
        
        for keyword, icon in icon_map.items():
            if keyword in title_lower:
                return icon
        
        return 'ğŸ“–'  # é»˜è®¤å›¾æ ‡


def safe_filename(title: str) -> str:
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '', title)
    filename = re.sub(r'\s+', '_', filename)
    filename = filename.strip('_')
    # é™åˆ¶é•¿åº¦
    if len(filename) > 100:
        filename = filename[:100]
    return filename or 'article'


def extract_article_section(html_content: str, section_class: str) -> List[str]:
    """ä»HTMLä¸­æå–æ–‡ç« åˆ—è¡¨éƒ¨åˆ†"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨å®¹å™¨
    article_list = soup.find(class_='article-list')
    if not article_list:
        return []
    
    # æå–æ‰€æœ‰æ–‡ç« é¡¹
    items = article_list.find_all(class_='article-list-item', recursive=False)
    return [str(item) for item in items]


def insert_article_to_list(html_content: str, article_html: str, max_items: int = None) -> str:
    """å°†æ–°æ–‡ç« æ’å…¥åˆ°æ–‡ç« åˆ—è¡¨çš„å¼€å¤´"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨å®¹å™¨
    article_list = soup.find(class_='article-list')
    if not article_list:
        return html_content
    
    # è§£ææ–°æ–‡ç« HTML
    new_article = BeautifulSoup(article_html, 'html.parser')
    new_item = new_article.find(class_='article-list-item')
    if not new_item:
        return html_content
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    
    # ç§»é™¤æ‰€æœ‰ç°æœ‰é¡¹
    for item in existing_items:
        item.decompose()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    article_list.insert(0, new_item)
    
    # é‡æ–°æ’å…¥ç°æœ‰æ–‡ç« ï¼ˆé™åˆ¶æ•°é‡ï¼‰
    if max_items:
        for item in existing_items[:max_items - 1]:
            article_list.append(item)
    else:
        for item in existing_items:
            article_list.append(item)
    
    return str(soup)


def generate_article_list_item(article: ArticleMetadata, target_dir: str, from_index: str = 'root') -> str:
    """ç”Ÿæˆæ–‡ç« åˆ—è¡¨é¡¹çš„HTML
    
    Args:
        article: æ–‡ç« å…ƒæ•°æ®
        target_dir: ç›®æ ‡ç›®å½• (blog/guides)
        from_index: ä»å“ªä¸ªç´¢å¼•é¡µè°ƒç”¨ ('root', 'blog', 'guides')
    """
    filename = safe_filename(article.title) + '.html'
    
    # æ ¹æ®è°ƒç”¨ä½ç½®ç¡®å®šç›¸å¯¹è·¯å¾„
    if from_index == 'root':
        # ä»index.htmlè°ƒç”¨ï¼Œéœ€è¦å®Œæ•´è·¯å¾„
        relative_path = f"{target_dir}/{filename}"
    else:
        # ä»blog/index.htmlæˆ–guides/index.htmlè°ƒç”¨ï¼Œåªéœ€è¦æ–‡ä»¶å
        relative_path = filename
    
    # æ ¹æ®ç±»åˆ«é€‰æ‹©æ ‡ç­¾æ ·å¼
    tag_class = "tag-story"
    if 'guide' in article.title.lower() or 'visa' in article.title.lower():
        tag_class = "tag-guide"
    elif 'food' in article.title.lower() or 'hotpot' in article.title.lower():
        tag_class = "tag-food"
    elif 'adventure' in article.title.lower() or 'hiking' in article.title.lower():
        tag_class = "tag-adventure"
    
    return f'''            <div class="article-list-item" onclick="window.location.href='{relative_path}'">
                <div class="article-icon">{article.get_icon()}</div>
                <div class="article-info">
                    <span class="article-tag {tag_class}">{article.category}</span>
                    <h3>{article.title}</h3>
                    <p>{article.description}</p>
                    <div class="article-meta-compact">
                        <span>ğŸ“… {article.date}</span>
                        <span>â±ï¸ {article.read_time}</span>
                        {f'<span>ğŸ“ {article.location}</span>' if article.location else ''}
                    </div>
                </div>
            </div>'''


def update_index_html(article: ArticleMetadata, target_dir: str, section_id: str):
    """æ›´æ–°index.htmlä¸­çš„æ–‡ç« åˆ—è¡¨ - ä¸»é¡µæ¯ä¸ªæ ç›®æœ€å¤šæ˜¾ç¤º5ç¯‡æ–‡ç« """
    if not INDEX_HTML.exists():
        print(f"âš ï¸  {INDEX_HTML} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    with open(INDEX_HTML, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾å¯¹åº”çš„section
    section = soup.find('section', id=section_id)
    if not section:
        print(f"âš ï¸  æœªæ‰¾åˆ°section #{section_id}")
        return
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
    article_list = section.find(class_='article-list')
    if not article_list:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
        return
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹å¹¶ä¿å­˜å®ƒä»¬çš„HTMLå­—ç¬¦ä¸²ï¼ˆåœ¨æ¸…é™¤ä¹‹å‰ï¼‰
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    existing_items_html = [str(item) for item in existing_items]
    
    # ç”Ÿæˆæ–°æ–‡ç« é¡¹ï¼ˆä»index.htmlè°ƒç”¨ï¼Œéœ€è¦å®Œæ•´è·¯å¾„ï¼‰
    article_html = generate_article_list_item(article, target_dir, from_index='root')
    
    # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
    article_list.clear()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    new_item = BeautifulSoup(article_html, 'html.parser')
    article_list.append(new_item)
    
    # é‡æ–°æ’å…¥ç°æœ‰æ–‡ç« ï¼ˆé™åˆ¶æ•°é‡ï¼šä¸»é¡µæœ€å¤šæ˜¾ç¤º5ç¯‡ï¼‰
    max_items = MAX_HOMEPAGE_STORIES if section_id == 'stories' else MAX_HOMEPAGE_GUIDES
    for item_html in existing_items_html[:max_items - 1]:
        existing_item = BeautifulSoup(item_html, 'html.parser')
        article_list.append(existing_item)
    
    # ä¿å­˜
    with open(INDEX_HTML, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²æ›´æ–° {INDEX_HTML} çš„ {section_id} éƒ¨åˆ†ï¼ˆæ˜¾ç¤ºæœ€æ–° {max_items} ç¯‡ï¼‰")


def update_blog_index(article: ArticleMetadata):
    """æ›´æ–°blog/index.html - å­é¡µé¢å¿…é¡»æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« """
    if not BLOG_INDEX.exists():
        print(f"âš ï¸  {BLOG_INDEX} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    with open(BLOG_INDEX, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
    article_list = soup.find(class_='article-list')
    if not article_list:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
        return
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹å¹¶ä¿å­˜å®ƒä»¬çš„HTMLå­—ç¬¦ä¸²ï¼ˆåœ¨æ¸…é™¤ä¹‹å‰ï¼‰
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    existing_items_html = [str(item) for item in existing_items]
    
    # ç”Ÿæˆæ–°æ–‡ç« é¡¹ï¼ˆä»blog/index.htmlè°ƒç”¨ï¼Œåªéœ€è¦æ–‡ä»¶åï¼‰
    article_html = generate_article_list_item(article, 'blog', from_index='blog')
    
    # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
    article_list.clear()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    new_item = BeautifulSoup(article_html, 'html.parser')
    article_list.append(new_item)
    
    # é‡æ–°æ’å…¥æ‰€æœ‰ç°æœ‰æ–‡ç« ï¼ˆå­é¡µé¢æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰
    for item_html in existing_items_html:
        existing_item = BeautifulSoup(item_html, 'html.parser')
        article_list.append(existing_item)
    
    # ä¿å­˜
    with open(BLOG_INDEX, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²æ›´æ–° {BLOG_INDEX}ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼‰")


def update_guides_index(article: ArticleMetadata):
    """æ›´æ–°guides/index.html - å­é¡µé¢å¿…é¡»æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« """
    if not GUIDES_INDEX.exists():
        print(f"âš ï¸  {GUIDES_INDEX} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    with open(GUIDES_INDEX, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ ¹æ®æ–‡ç« å†…å®¹åˆ¤æ–­åº”è¯¥æ”¾åœ¨å“ªä¸ªsection
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ”¾åœ¨ç¬¬ä¸€ä¸ªåˆé€‚çš„section
    sections = soup.find_all('section')
    target_section = None
    
    title_lower = article.title.lower()
    if 'visa' in title_lower:
        target_section = soup.find('section', id='visa')
    elif any(word in title_lower for word in ['train', 'rail', 'transport', 'metro', 'didi']):
        target_section = soup.find('section', id='transport')
    elif any(word in title_lower for word in ['app', 'vpn', 'internet', 'wechat', 'alipay']):
        target_section = soup.find('section', id='tech')
    elif any(word in title_lower for word in ['food', 'dining', 'restaurant', 'ordering']):
        target_section = soup.find('section', id='food')
    elif any(word in title_lower for word in ['city', 'beijing', 'shanghai', 'chengdu']):
        target_section = soup.find('section', id='cities')
    
    # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šsectionï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªsection
    if not target_section and sections:
        target_section = sections[0]
    
    if not target_section:
        print(f"âš ï¸  æœªæ‰¾åˆ°åˆé€‚çš„section")
        return
    
    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
    article_list = target_section.find(class_='article-list')
    if not article_list:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
        return
    
    # è·å–ç°æœ‰æ–‡ç« é¡¹å¹¶ä¿å­˜å®ƒä»¬çš„HTMLå­—ç¬¦ä¸²ï¼ˆåœ¨æ¸…é™¤ä¹‹å‰ï¼‰
    existing_items = article_list.find_all(class_='article-list-item', recursive=False)
    existing_items_html = [str(item) for item in existing_items]
    
    # ç”Ÿæˆæ–°æ–‡ç« é¡¹ï¼ˆä»guides/index.htmlè°ƒç”¨ï¼Œåªéœ€è¦æ–‡ä»¶åï¼‰
    article_html = generate_article_list_item(article, 'guides', from_index='guides')
    
    # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
    article_list.clear()
    
    # æ’å…¥æ–°æ–‡ç« åˆ°å¼€å¤´
    new_item = BeautifulSoup(article_html, 'html.parser')
    article_list.append(new_item)
    
    # é‡æ–°æ’å…¥æ‰€æœ‰ç°æœ‰æ–‡ç« ï¼ˆå­é¡µé¢æ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰
    for item_html in existing_items_html:
        existing_item = BeautifulSoup(item_html, 'html.parser')
        article_list.append(existing_item)
    
    # ä¿å­˜
    with open(GUIDES_INDEX, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²æ›´æ–° {GUIDES_INDEX}ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ–‡ç« ï¼‰")


def fix_article_paths(html_file: Path, target_dir: str):
    """ä¿®å¤æ–‡ç« ä¸­çš„ç›¸å¯¹è·¯å¾„"""
    with open(html_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    soup = BeautifulSoup(content, 'html.parser')
    
    # ä¿®å¤å¯¼èˆªé“¾æ¥
    # blogå’Œguidesç›®å½•ä¸­çš„æ–‡ç« éƒ½éœ€è¦../è¿”å›æ ¹ç›®å½•
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        if not href or href.startswith('http') or href.startswith('#'):
            continue  # è·³è¿‡å¤–éƒ¨é“¾æ¥å’Œé”šç‚¹
        
        # ä¿®å¤index.htmlé“¾æ¥
        if href == 'index.html' or href.endswith('/index.html'):
            link['href'] = '../index.html'
        # ä¿®å¤blog/index.htmlå’Œguides/index.htmlé“¾æ¥
        elif href == 'blog/index.html':
            link['href'] = '../blog/index.html'
        elif href == 'guides/index.html':
            link['href'] = '../guides/index.html'
        # ä¿®å¤å…¶ä»–ç›¸å¯¹è·¯å¾„
        elif href.startswith('blog/') and not href.startswith('../blog/'):
            link['href'] = '../' + href
        elif href.startswith('guides/') and not href.startswith('../guides/'):
            link['href'] = '../' + href
    
    # ä¿®å¤logoé“¾æ¥
    logo = soup.find(class_='logo')
    if logo and logo.get('href'):
        if logo['href'] == 'index.html':
            logo['href'] = '../index.html'
    
    # ä¿å­˜
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… å·²ä¿®å¤è·¯å¾„: {html_file.name}")


def deploy_article(source_file: Path, target_dir: str, auto_detect: bool = False) -> bool:
    """éƒ¨ç½²å•ç¯‡æ–‡ç« """
    try:
        # æå–å…ƒæ•°æ®
        article = ArticleMetadata(source_file)
        if not article.extract_from_html():
            return False
        
        # è‡ªåŠ¨åˆ¤æ–­ç›®æ ‡ç›®å½•
        if auto_detect:
            target_dir = article.determine_category()
        
        # ç¡®å®šç›®æ ‡ç›®å½•
        if target_dir == 'blog':
            target_path = BLOG_DIR
            section_id = 'stories'
        elif target_dir == 'guides':
            target_path = GUIDES_DIR
            section_id = 'guides'
        else:
            print(f"âŒ æ— æ•ˆçš„ç›®æ ‡ç›®å½•: {target_dir}")
            return False
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        target_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç›®æ ‡æ–‡ä»¶å
        target_filename = safe_filename(article.title) + '.html'
        target_file = target_path / target_filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯¢é—®æ˜¯å¦è¦†ç›–
        if target_file.exists():
            print(f"âš ï¸  æ–‡ä»¶å·²å­˜åœ¨: {target_file}")
            response = input("æ˜¯å¦è¦†ç›–? (y/n): ").strip().lower()
            if response != 'y':
                print("â­ï¸  è·³è¿‡æ­¤æ–‡ä»¶")
                return False
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(source_file, target_file)
        print(f"âœ… å·²å¤åˆ¶æ–‡ä»¶: {target_file}")
        
        # ä¿®å¤æ–‡ç« ä¸­çš„è·¯å¾„
        fix_article_paths(target_file, target_dir)
        
        # æ›´æ–°ç´¢å¼•é¡µ
        if target_dir == 'blog':
            update_blog_index(article)
        else:
            update_guides_index(article)
        
        # æ›´æ–°ä¸»é¡µ
        update_index_html(article, target_dir, section_id)
        
        print(f"âœ… æˆåŠŸéƒ¨ç½²: {article.title}")
        return True
        
    except Exception as e:
        print(f"âŒ éƒ¨ç½²å¤±è´¥ {source_file}: {e}")
        import traceback
        traceback.print_exc()
        return False


def rebuild_index(target_dir: str):
    """ä»æ–‡ä»¶ç³»ç»Ÿæ‰«ææ‰€æœ‰æ–‡ç« å¹¶é‡å»ºç´¢å¼•"""
    if target_dir == 'blog':
        target_path = BLOG_DIR
        index_file = BLOG_INDEX
    elif target_dir == 'guides':
        target_path = GUIDES_DIR
        index_file = GUIDES_INDEX
    else:
        print(f"âŒ æ— æ•ˆçš„ç›®æ ‡ç›®å½•: {target_dir}")
        return
    
    if not target_path.exists():
        print(f"âŒ ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_path}")
        return
    
    if not index_file.exists():
        print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
        return
    
    # æ‰«ææ‰€æœ‰HTMLæ–‡ä»¶ï¼ˆæ’é™¤index.htmlï¼‰
    article_files = [f for f in target_path.glob('*.html') if f.name != 'index.html']
    
    if not article_files:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç« æ–‡ä»¶: {target_path}")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(article_files)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹é‡å»ºç´¢å¼•...")
    
    # æå–æ‰€æœ‰æ–‡ç« çš„å…ƒæ•°æ®
    articles = []
    for article_file in article_files:
        article = ArticleMetadata(article_file)
        if article.extract_from_html():
            articles.append(article)
            print(f"  âœ“ {article.title}")
        else:
            print(f"  âœ— è·³è¿‡ {article_file.name}ï¼ˆæ— æ³•æå–å…ƒæ•°æ®ï¼‰")
    
    if not articles:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ç« å¯ä»¥æ·»åŠ åˆ°ç´¢å¼•")
        return
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    articles.sort(key=lambda x: x.date, reverse=True)
    
    # è¯»å–ç´¢å¼•æ–‡ä»¶
    with open(index_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    if target_dir == 'blog':
        # blog/index.html åªæœ‰ä¸€ä¸ª article-list
        article_list = soup.find(class_='article-list')
        if article_list:
            # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
            article_list.clear()
            
            # æ·»åŠ æ‰€æœ‰æ–‡ç« 
            for article in articles:
                article_html = generate_article_list_item(article, 'blog', from_index='blog')
                article_item = BeautifulSoup(article_html, 'html.parser')
                article_list.append(article_item)
            
            print(f"âœ… å·²é‡å»º {index_file}ï¼ŒåŒ…å« {len(articles)} ç¯‡æ–‡ç« ")
    else:
        # guides/index.html æœ‰å¤šä¸ª sectionï¼Œæ¯ä¸ª section æœ‰è‡ªå·±çš„ article-list
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå°†æ‰€æœ‰æ–‡ç« æ·»åŠ åˆ°ç¬¬ä¸€ä¸ªåˆé€‚çš„ section
        sections = soup.find_all('section')
        if sections:
            # ä¸ºæ¯ç¯‡æ–‡ç« æ‰¾åˆ°åˆé€‚çš„ section
            for article in articles:
                title_lower = article.title.lower()
                target_section = None
                
                if 'visa' in title_lower:
                    target_section = soup.find('section', id='visa')
                elif any(word in title_lower for word in ['train', 'rail', 'transport', 'metro', 'didi']):
                    target_section = soup.find('section', id='transport')
                elif any(word in title_lower for word in ['app', 'vpn', 'internet', 'wechat', 'alipay']):
                    target_section = soup.find('section', id='tech')
                elif any(word in title_lower for word in ['food', 'dining', 'restaurant', 'ordering']):
                    target_section = soup.find('section', id='food')
                elif any(word in title_lower for word in ['city', 'beijing', 'shanghai', 'chengdu']):
                    target_section = soup.find('section', id='cities')
                
                if not target_section and sections:
                    target_section = sections[0]
                
                if target_section:
                    article_list = target_section.find(class_='article-list')
                    if article_list:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
                        existing_titles = [item.find('h3').get_text() if item.find('h3') else '' 
                                          for item in article_list.find_all(class_='article-list-item', recursive=False)]
                        if article.title not in existing_titles:
                            article_html = generate_article_list_item(article, 'guides', from_index='guides')
                            article_item = BeautifulSoup(article_html, 'html.parser')
                            article_list.append(article_item)
            
            print(f"âœ… å·²é‡å»º {index_file}ï¼ŒåŒ…å« {len(articles)} ç¯‡æ–‡ç« ")
    
    # ä¿å­˜
    with open(index_file, 'w', encoding='utf-8') as f:
        f.write(str(soup))


def rebuild_homepage():
    """é‡å»ºä¸»é¡µé¢çš„ç´¢å¼•ï¼ˆä»blogå’Œguidesç›®å½•æ‰«ææœ€æ–°æ–‡ç« ï¼‰"""
    if not INDEX_HTML.exists():
        print(f"âŒ ä¸»é¡µé¢ä¸å­˜åœ¨: {INDEX_HTML}")
        return
    
    print("ğŸ”„ å¼€å§‹é‡å»ºä¸»é¡µé¢ç´¢å¼•...")
    
    # æ‰«æ blog ç›®å½•çš„æ–‡ç« 
    blog_articles = []
    if BLOG_DIR.exists():
        blog_files = [f for f in BLOG_DIR.glob('*.html') if f.name != 'index.html']
        print(f"\nğŸ“ æ‰«æ blog ç›®å½•ï¼Œæ‰¾åˆ° {len(blog_files)} ç¯‡æ–‡ç« ")
        for article_file in blog_files:
            article = ArticleMetadata(article_file)
            if article.extract_from_html():
                blog_articles.append(article)
    
    # æ‰«æ guides ç›®å½•çš„æ–‡ç« 
    guides_articles = []
    if GUIDES_DIR.exists():
        guides_files = [f for f in GUIDES_DIR.glob('*.html') if f.name != 'index.html']
        print(f"ğŸ“ æ‰«æ guides ç›®å½•ï¼Œæ‰¾åˆ° {len(guides_files)} ç¯‡æ–‡ç« ")
        for article_file in guides_files:
            article = ArticleMetadata(article_file)
            if article.extract_from_html():
                guides_articles.append(article)
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    blog_articles.sort(key=lambda x: x.date, reverse=True)
    guides_articles.sort(key=lambda x: x.date, reverse=True)
    
    # è¯»å–ä¸»é¡µé¢
    with open(INDEX_HTML, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ›´æ–° stories section
    stories_section = soup.find('section', id='stories')
    if stories_section:
        article_list = stories_section.find(class_='article-list')
        if article_list:
            # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
            article_list.clear()
            
            # æ·»åŠ æœ€æ–°çš„5ç¯‡ blog æ–‡ç« 
            for article in blog_articles[:MAX_HOMEPAGE_STORIES]:
                article_html = generate_article_list_item(article, 'blog', from_index='root')
                article_item = BeautifulSoup(article_html, 'html.parser')
                article_list.append(article_item)
            
            print(f"âœ… å·²æ›´æ–° stories sectionï¼Œæ˜¾ç¤º {min(len(blog_articles), MAX_HOMEPAGE_STORIES)} ç¯‡æ–‡ç« ")
    
    # æ›´æ–° guides section
    guides_section = soup.find('section', id='guides')
    if guides_section:
        article_list = guides_section.find(class_='article-list')
        if article_list:
            # æ¸…é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« é¡¹å’Œæ–‡æœ¬èŠ‚ç‚¹ï¼‰
            article_list.clear()
            
            # æ·»åŠ æœ€æ–°çš„5ç¯‡ guides æ–‡ç« 
            for article in guides_articles[:MAX_HOMEPAGE_GUIDES]:
                article_html = generate_article_list_item(article, 'guides', from_index='root')
                article_item = BeautifulSoup(article_html, 'html.parser')
                article_list.append(article_item)
            
            print(f"âœ… å·²æ›´æ–° guides sectionï¼Œæ˜¾ç¤º {min(len(guides_articles), MAX_HOMEPAGE_GUIDES)} ç¯‡æ–‡ç« ")
    
    # ä¿å­˜
    with open(INDEX_HTML, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"\nâœ… å·²é‡å»ºä¸»é¡µé¢ç´¢å¼•")


def deploy_all(source_dir: Path, target_dir: str = None, auto_detect: bool = False):
    """éƒ¨ç½²æ‰€æœ‰æ–‡ç« """
    if not source_dir.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶
    html_files = list(source_dir.glob('*.html'))
    if not html_files:
        print(f"âš ï¸  æœªæ‰¾åˆ°HTMLæ–‡ä»¶: {source_dir}")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    success_count = 0
    for html_file in html_files:
        print(f"\nğŸ“„ å¤„ç†: {html_file.name}")
        if deploy_article(html_file, target_dir or 'blog', auto_detect):
            success_count += 1
    
    print(f"\n{'='*50}")
    print(f"âœ… æˆåŠŸéƒ¨ç½²: {success_count}/{len(html_files)}")
    print(f"{'='*50}")


def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨éƒ¨ç½²ç¿»è¯‘åçš„æ–‡ç« ')
    parser.add_argument('--source-dir', '-s', 
                       default=str(TRANSLATED_DIR),
                       help='æºç›®å½•ï¼ˆåŒ…å«ç¿»è¯‘åçš„HTMLæ–‡ä»¶ï¼‰')
    parser.add_argument('--target', '-t', 
                       choices=['blog', 'guides'],
                       help='ç›®æ ‡ç›®å½•ï¼ˆblogæˆ–guidesï¼‰')
    parser.add_argument('--auto', '-a', 
                       action='store_true',
                       help='è‡ªåŠ¨åˆ¤æ–­æ˜¯blogè¿˜æ˜¯guides')
    parser.add_argument('--file', '-f',
                       help='éƒ¨ç½²å•ä¸ªæ–‡ä»¶ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªç›®å½•ï¼‰')
    parser.add_argument('--rebuild', '-r',
                       choices=['blog', 'guides', 'homepage'],
                       help='ä»æ–‡ä»¶ç³»ç»Ÿé‡å»ºç´¢å¼•ï¼ˆblogã€guidesæˆ–homepageï¼‰')
    
    args = parser.parse_args()
    
    source_dir = Path(args.source_dir)
    
    # å¦‚æœæŒ‡å®šäº†é‡å»ºç´¢å¼•
    if args.rebuild:
        if args.rebuild == 'homepage':
            rebuild_homepage()
        else:
            rebuild_index(args.rebuild)
        return
    
    # å¦‚æœæŒ‡å®šäº†å•ä¸ªæ–‡ä»¶
    if args.file:
        source_file = Path(args.file)
        if not source_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
            sys.exit(1)
        
        target_dir = args.target or 'blog'
        auto_detect = args.auto
        deploy_article(source_file, target_dir, auto_detect)
    else:
        # éƒ¨ç½²æ•´ä¸ªç›®å½•
        if args.auto:
            deploy_all(source_dir, auto_detect=True)
        elif args.target:
            deploy_all(source_dir, target_dir=args.target)
        else:
            print("âŒ è¯·æŒ‡å®š --target (blog/guides) æˆ–ä½¿ç”¨ --auto è‡ªåŠ¨åˆ¤æ–­")
            sys.exit(1)


if __name__ == '__main__':
    main()